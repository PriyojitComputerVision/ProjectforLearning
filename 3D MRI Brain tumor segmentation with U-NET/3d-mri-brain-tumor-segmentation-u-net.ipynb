{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Problem definiton\n","**Segmentation of gliomas in pre-operative MRI scans.**\n","\n","*Each pixel on image must be labeled:*\n","* Pixel is part of a tumor area (1 or 2 or 3) -> can be one of multiple classes / sub-regions\n","* Anything else -> pixel is not on a tumor region (0)\n","\n","The sub-regions of tumor considered for evaluation are: 1) the \"enhancing tumor\" (ET), 2) the \"tumor core\" (TC), and 3) the \"whole tumor\" (WT)\n","The provided segmentation labels have values of 1 for NCR & NET, 2 for ED, 4 for ET, and 0 for everything else.\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["![Brats official annotations](https://www.med.upenn.edu/cbica/assets/user-content/images/BraTS/brats-tumor-subregions.jpg)"]},{"cell_type":"markdown","metadata":{},"source":["# Setup env"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import os\n","import cv2\n","import glob\n","import PIL\n","import shutil\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from skimage import data\n","from skimage.util import montage \n","import skimage.transform as skTrans\n","from skimage.transform import rotate\n","from skimage.transform import resize\n","from PIL import Image, ImageOps  \n","\n","\n","# neural imaging\n","import nilearn as nl\n","import nibabel as nib\n","import nilearn.plotting as nlplt\n","!pip install git+https://github.com/miykael/gif_your_nifti # nifti to gif \n","import gif_your_nifti.core as gif2nif\n","\n","\n","# ml libs\n","import keras\n","import keras.backend as K\n","from keras.callbacks import CSVLogger\n","import tensorflow as tf\n","from tensorflow.keras.utils import plot_model\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","from tensorflow.keras.models import *\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.optimizers import *\n","from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard\n","from tensorflow.keras.layers.experimental import preprocessing\n","\n","\n","# Make numpy printouts easier to read.\n","np.set_printoptions(precision=3, suppress=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# DEFINE seg-areas  \n","SEGMENT_CLASSES = {\n","    0 : 'NOT tumor',\n","    1 : 'NECROTIC/CORE', # or NON-ENHANCING tumor CORE\n","    2 : 'EDEMA',\n","    3 : 'ENHANCING' # original 4 -> converted into 3 later\n","}\n","\n","# there are 155 slices per volume\n","# to start at 5 and use 145 slices means we will skip the first 5 and last 5 \n","VOLUME_SLICES = 100 \n","VOLUME_START_AT = 22 # first slice of volume that we will include"]},{"cell_type":"markdown","metadata":{},"source":["# Image data descriptions\n","\n","All BraTS multimodal scans are available as  NIfTI files (.nii.gz) -> commonly used medical imaging format to store brain imagin data obtained using MRI and describe different MRI settings \n","1. **T1**: T1-weighted, native image, sagittal or axial 2D acquisitions, with 1–6 mm slice thickness.\n","2. **T1c**: T1-weighted, contrast-enhanced (Gadolinium) image, with 3D acquisition and 1 mm isotropic voxel size for most patients.\n","3. **T2**: T2-weighted image, axial 2D acquisition, with 2–6 mm slice thickness.\n","4. **FLAIR**: T2-weighted FLAIR image, axial, coronal, or sagittal 2D acquisitions, 2–6 mm slice thickness.\n","\n","Data were acquired with different clinical protocols and various scanners from multiple (n=19) institutions.\n","\n","All the imaging datasets have been segmented manually, by one to four raters, following the same annotation protocol, and their annotations were approved by experienced neuro-radiologists. Annotations comprise the GD-enhancing tumor (ET — label 4), the peritumoral edema (ED — label 2), and the necrotic and non-enhancing tumor core (NCR/NET — label 1), as described both in the BraTS 2012-2013 TMI paper and in the latest BraTS summarizing paper. The provided data are distributed after their pre-processing, i.e., co-registered to the same anatomical template, interpolated to the same resolution (1 mm^3) and skull-stripped.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["TRAIN_DATASET_PATH = '../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/'\n","VALIDATION_DATASET_PATH = '../input/brats20-dataset-training-validation/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData'\n","\n","test_image_flair=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_flair.nii').get_fdata()\n","test_image_t1=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_t1.nii').get_fdata()\n","test_image_t1ce=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_t1ce.nii').get_fdata()\n","test_image_t2=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_t2.nii').get_fdata()\n","test_mask=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_seg.nii').get_fdata()\n","\n","\n","fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1,5, figsize = (20, 10))\n","slice_w = 25\n","ax1.imshow(test_image_flair[:,:,test_image_flair.shape[0]//2-slice_w], cmap = 'gray')\n","ax1.set_title('Image flair')\n","ax2.imshow(test_image_t1[:,:,test_image_t1.shape[0]//2-slice_w], cmap = 'gray')\n","ax2.set_title('Image t1')\n","ax3.imshow(test_image_t1ce[:,:,test_image_t1ce.shape[0]//2-slice_w], cmap = 'gray')\n","ax3.set_title('Image t1ce')\n","ax4.imshow(test_image_t2[:,:,test_image_t2.shape[0]//2-slice_w], cmap = 'gray')\n","ax4.set_title('Image t2')\n","ax5.imshow(test_mask[:,:,test_mask.shape[0]//2-slice_w])\n","ax5.set_title('Mask')\n"]},{"cell_type":"markdown","metadata":{},"source":["**Show whole nifti data -> print each slice from 3d data**"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["# Skip 50:-50 slices since there is not much to see\n","fig, ax1 = plt.subplots(1, 1, figsize = (15,15))\n","ax1.imshow(rotate(montage(test_image_t1[50:-50,:,:]), 90, resize=True), cmap ='gray')"]},{"cell_type":"markdown","metadata":{},"source":["**Show segment of tumor for each above slice**"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["# Skip 50:-50 slices since there is not much to see\n","fig, ax1 = plt.subplots(1, 1, figsize = (15,15))\n","ax1.imshow(rotate(montage(test_mask[60:-60,:,:]), 90, resize=True), cmap ='gray')"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["shutil.copy2(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_flair.nii', './test_gif_BraTS20_Training_001_flair.nii')\n","gif2nif.write_gif_normal('./test_gif_BraTS20_Training_001_flair.nii')"]},{"cell_type":"markdown","metadata":{},"source":["**Gif representation of slices in 3D volume**\n","<img src=\"https://media1.tenor.com/images/15427ffc1399afc3334f12fd27549a95/tenor.gif?itemid=20554734\">"]},{"cell_type":"markdown","metadata":{},"source":["**Show segments of tumor using different effects**"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["niimg = nl.image.load_img(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_flair.nii')\n","nimask = nl.image.load_img(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_seg.nii')\n","\n","fig, axes = plt.subplots(nrows=4, figsize=(30, 40))\n","\n","\n","nlplt.plot_anat(niimg,\n","                title='BraTS20_Training_001_flair.nii plot_anat',\n","                axes=axes[0])\n","\n","nlplt.plot_epi(niimg,\n","               title='BraTS20_Training_001_flair.nii plot_epi',\n","               axes=axes[1])\n","\n","nlplt.plot_img(niimg,\n","               title='BraTS20_Training_001_flair.nii plot_img',\n","               axes=axes[2])\n","\n","nlplt.plot_roi(nimask, \n","               title='BraTS20_Training_001_flair.nii with mask plot_roi',\n","               bg_img=niimg, \n","               axes=axes[3], cmap='Paired')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Create model || U-Net: Convolutional Networks for Biomedical Image Segmentation\n","he u-net is convolutional network architecture for fast and precise segmentation of images. Up to now it has outperformed the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. It has won the Grand Challenge for Computer-Automated Detection of Caries in Bitewing Radiography at ISBI 2015, and it has won the Cell Tracking Challenge at ISBI 2015 on the two most challenging transmitted light microscopy categories (Phase contrast and DIC microscopy) by a large margin\n","[more on](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/)\n","![official definiton](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Loss function\n","**Dice coefficient**\n",", which is essentially a measure of overlap between two samples. This measure ranges from 0 to 1 where a Dice coefficient of 1 denotes perfect and complete overlap. The Dice coefficient was originally developed for binary data, and can be calculated as:\n","\n","![dice loss](https://wikimedia.org/api/rest_v1/media/math/render/svg/a80a97215e1afc0b222e604af1b2099dc9363d3b)\n","\n","**As matrices**\n","![dice loss](https://www.jeremyjordan.me/content/images/2018/05/intersection-1.png)\n","\n","[Implementation, (images above) and explanation can be found here](https://www.jeremyjordan.me/semantic-segmentation/)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# dice loss as defined above for 4 classes\n","def dice_coef(y_true, y_pred, smooth=1.0):\n","    class_num = 4\n","    for i in range(class_num):\n","        y_true_f = K.flatten(y_true[:,:,:,i])\n","        y_pred_f = K.flatten(y_pred[:,:,:,i])\n","        intersection = K.sum(y_true_f * y_pred_f)\n","        loss = ((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))\n","   #     K.print_tensor(loss, message='loss value for class {} : '.format(SEGMENT_CLASSES[i]))\n","        if i == 0:\n","            total_loss = loss\n","        else:\n","            total_loss = total_loss + loss\n","    total_loss = total_loss / class_num\n","#    K.print_tensor(total_loss, message=' total dice coef: ')\n","    return total_loss\n","\n","\n"," \n","# define per class evaluation of dice coef\n","# inspired by https://github.com/keras-team/keras/issues/9395\n","def dice_coef_necrotic(y_true, y_pred, epsilon=1e-6):\n","    intersection = K.sum(K.abs(y_true[:,:,:,1] * y_pred[:,:,:,1]))\n","    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,1])) + K.sum(K.square(y_pred[:,:,:,1])) + epsilon)\n","\n","def dice_coef_edema(y_true, y_pred, epsilon=1e-6):\n","    intersection = K.sum(K.abs(y_true[:,:,:,2] * y_pred[:,:,:,2]))\n","    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,2])) + K.sum(K.square(y_pred[:,:,:,2])) + epsilon)\n","\n","def dice_coef_enhancing(y_true, y_pred, epsilon=1e-6):\n","    intersection = K.sum(K.abs(y_true[:,:,:,3] * y_pred[:,:,:,3]))\n","    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,3])) + K.sum(K.square(y_pred[:,:,:,3])) + epsilon)\n","\n","\n","\n","# Computing Precision \n","def precision(y_true, y_pred):\n","        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","        precision = true_positives / (predicted_positives + K.epsilon())\n","        return precision\n","\n","    \n","# Computing Sensitivity      \n","def sensitivity(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    return true_positives / (possible_positives + K.epsilon())\n","\n","\n","# Computing Specificity\n","def specificity(y_true, y_pred):\n","    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n","    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n","    return true_negatives / (possible_negatives + K.epsilon())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["IMG_SIZE=128"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["# source https://naomi-fridman.medium.com/multi-class-image-segmentation-a5cc671e647a\n","\n","def build_unet(inputs, ker_init, dropout):\n","    conv1 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(inputs)\n","    conv1 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv1)\n","    \n","    pool = MaxPooling2D(pool_size=(2, 2))(conv1)\n","    conv = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool)\n","    conv = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv)\n","    \n","    pool1 = MaxPooling2D(pool_size=(2, 2))(conv)\n","    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool1)\n","    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv2)\n","    \n","    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n","    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool2)\n","    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv3)\n","    \n","    \n","    pool4 = MaxPooling2D(pool_size=(2, 2))(conv3)\n","    conv5 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool4)\n","    conv5 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv5)\n","    drop5 = Dropout(dropout)(conv5)\n","\n","    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(drop5))\n","    merge7 = concatenate([conv3,up7], axis = 3)\n","    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge7)\n","    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv7)\n","\n","    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(conv7))\n","    merge8 = concatenate([conv2,up8], axis = 3)\n","    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge8)\n","    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv8)\n","\n","    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(conv8))\n","    merge9 = concatenate([conv,up9], axis = 3)\n","    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge9)\n","    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv9)\n","    \n","    up = Conv2D(32, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(conv9))\n","    merge = concatenate([conv1,up], axis = 3)\n","    conv = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge)\n","    conv = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv)\n","    \n","    conv10 = Conv2D(4, (1,1), activation = 'softmax')(conv)\n","    \n","    return Model(inputs = inputs, outputs = conv10)\n","\n","input_layer = Input((IMG_SIZE, IMG_SIZE, 2))\n","\n","model = build_unet(input_layer, 'he_normal', 0.2)\n","model.compile(loss=\"categorical_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics = ['accuracy',tf.keras.metrics.MeanIoU(num_classes=4), dice_coef, precision, sensitivity, specificity, dice_coef_necrotic, dice_coef_edema ,dice_coef_enhancing] )"]},{"cell_type":"markdown","metadata":{},"source":["**model architecture** <br>\n","If you are about to use U-NET, I suggest to check out this awesome library that I found later, after manual implementation of U-NET [keras-unet-collection](https://pypi.org/project/keras-unet-collection/), which also contains implementation of dice loss, tversky loss and many more!"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plot_model(model, \n","           show_shapes = True,\n","           show_dtype=False,\n","           show_layer_names = True, \n","           rankdir = 'TB', \n","           expand_nested = False, \n","           dpi = 70)"]},{"cell_type":"markdown","metadata":{},"source":["# Load data\n","Loading all data into memory is not a good idea since the data are too big to fit in.\n","So we will create dataGenerators - load data on the fly as explained [here](https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly)"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["# lists of directories with studies\n","train_and_val_directories = [f.path for f in os.scandir(TRAIN_DATASET_PATH) if f.is_dir()]\n","\n","# file BraTS20_Training_355 has ill formatted name for for seg.nii file\n","train_and_val_directories.remove(TRAIN_DATASET_PATH+'BraTS20_Training_355')\n","\n","\n","def pathListIntoIds(dirList):\n","    x = []\n","    for i in range(0,len(dirList)):\n","        x.append(dirList[i][dirList[i].rfind('/')+1:])\n","    return x\n","\n","train_and_test_ids = pathListIntoIds(train_and_val_directories); \n","\n","    \n","train_test_ids, val_ids = train_test_split(train_and_test_ids,test_size=0.2) \n","train_ids, test_ids = train_test_split(train_test_ids,test_size=0.15) "]},{"cell_type":"markdown","metadata":{},"source":["**Override Keras sequence DataGenerator class**"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["class DataGenerator(keras.utils.Sequence):\n","    'Generates data for Keras'\n","    def __init__(self, list_IDs, dim=(IMG_SIZE,IMG_SIZE), batch_size = 1, n_channels = 2, shuffle=True):\n","        'Initialization'\n","        self.dim = dim\n","        self.batch_size = batch_size\n","        self.list_IDs = list_IDs\n","        self.n_channels = n_channels\n","        self.shuffle = shuffle\n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        'Denotes the number of batches per epoch'\n","        return int(np.floor(len(self.list_IDs) / self.batch_size))\n","\n","    def __getitem__(self, index):\n","        'Generate one batch of data'\n","        # Generate indexes of the batch\n","        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n","\n","        # Find list of IDs\n","        Batch_ids = [self.list_IDs[k] for k in indexes]\n","\n","        # Generate data\n","        X, y = self.__data_generation(Batch_ids)\n","\n","        return X, y\n","\n","    def on_epoch_end(self):\n","        'Updates indexes after each epoch'\n","        self.indexes = np.arange(len(self.list_IDs))\n","        if self.shuffle == True:\n","            np.random.shuffle(self.indexes)\n","\n","    def __data_generation(self, Batch_ids):\n","        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n","        # Initialization\n","        X = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, self.n_channels))\n","        y = np.zeros((self.batch_size*VOLUME_SLICES, 240, 240))\n","        Y = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, 4))\n","\n","        \n","        # Generate data\n","        for c, i in enumerate(Batch_ids):\n","            case_path = os.path.join(TRAIN_DATASET_PATH, i)\n","\n","            data_path = os.path.join(case_path, f'{i}_flair.nii');\n","            flair = nib.load(data_path).get_fdata()    \n","\n","            data_path = os.path.join(case_path, f'{i}_t1ce.nii');\n","            ce = nib.load(data_path).get_fdata()\n","            \n","            data_path = os.path.join(case_path, f'{i}_seg.nii');\n","            seg = nib.load(data_path).get_fdata()\n","        \n","            for j in range(VOLUME_SLICES):\n","                 X[j +VOLUME_SLICES*c,:,:,0] = cv2.resize(flair[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n","                 X[j +VOLUME_SLICES*c,:,:,1] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n","\n","                 y[j +VOLUME_SLICES*c] = seg[:,:,j+VOLUME_START_AT];\n","                    \n","        # Generate masks\n","        y[y==4] = 3;\n","        mask = tf.one_hot(y, 4);\n","        Y = tf.image.resize(mask, (IMG_SIZE, IMG_SIZE));\n","        return X/np.max(X), Y\n","        \n","training_generator = DataGenerator(train_ids)\n","valid_generator = DataGenerator(val_ids)\n","test_generator = DataGenerator(test_ids)"]},{"cell_type":"markdown","metadata":{},"source":["**Number of data used**\n","for training / testing / validation"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["# show number of data for each dir \n","def showDataLayout():\n","    plt.bar([\"Train\",\"Valid\",\"Test\"],\n","    [len(train_ids), len(val_ids), len(test_ids)], align='center',color=[ 'green','red', 'blue'])\n","    plt.legend()\n","\n","    plt.ylabel('Number of images')\n","    plt.title('Data distribution')\n","\n","    plt.show()\n","    \n","showDataLayout()"]},{"cell_type":"markdown","metadata":{},"source":["**Add callback for training process**"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["csv_logger = CSVLogger('training.log', separator=',', append=False)\n","\n","\n","callbacks = [\n","#     keras.callbacks.EarlyStopping(monitor='loss', min_delta=0,\n","#                               patience=2, verbose=1, mode='auto'),\n","      keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n","                              patience=2, min_lr=0.000001, verbose=1),\n","#  keras.callbacks.ModelCheckpoint(filepath = 'model_.{epoch:02d}-{val_loss:.6f}.m5',\n","#                             verbose=1, save_best_only=True, save_weights_only = True)\n","        csv_logger\n","    ]"]},{"cell_type":"markdown","metadata":{},"source":["# Train model\n","My best model was trained with 81% accuracy on mean IOU and 65.5% on Dice loss <br>\n","I will load this pretrained model instead of training again"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["K.clear_session()\n","\n","# history =  model.fit(training_generator,\n","#                     epochs=35,\n","#                     steps_per_epoch=len(train_ids),\n","#                     callbacks= callbacks,\n","#                     validation_data = valid_generator\n","#                     )  \n","# model.save(\"model_x1_1.h5\")"]},{"cell_type":"markdown","metadata":{},"source":["**Visualize the training process**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["############ load trained model ################\n","model = keras.models.load_model('../input/modelperclasseval/model_per_class.h5', \n","                                   custom_objects={ 'accuracy' : tf.keras.metrics.MeanIoU(num_classes=4),\n","                                                   \"dice_coef\": dice_coef,\n","                                                   \"precision\": precision,\n","                                                   \"sensitivity\":sensitivity,\n","                                                   \"specificity\":specificity,\n","                                                   \"dice_coef_necrotic\": dice_coef_necrotic,\n","                                                   \"dice_coef_edema\": dice_coef_edema,\n","                                                   \"dice_coef_enhancing\": dice_coef_enhancing\n","                                                  }, compile=False)\n","\n","history = pd.read_csv('../input/modelperclasseval/training_per_class.log', sep=',', engine='python')\n","\n","hist=history\n","\n","############### ########## ####### #######\n","\n","# hist=history.history\n","\n","acc=hist['accuracy']\n","val_acc=hist['val_accuracy']\n","\n","epoch=range(len(acc))\n","\n","loss=hist['loss']\n","val_loss=hist['val_loss']\n","\n","train_dice=hist['dice_coef']\n","val_dice=hist['val_dice_coef']\n","\n","f,ax=plt.subplots(1,4,figsize=(16,8))\n","\n","ax[0].plot(epoch,acc,'b',label='Training Accuracy')\n","ax[0].plot(epoch,val_acc,'r',label='Validation Accuracy')\n","ax[0].legend()\n","\n","ax[1].plot(epoch,loss,'b',label='Training Loss')\n","ax[1].plot(epoch,val_loss,'r',label='Validation Loss')\n","ax[1].legend()\n","\n","ax[2].plot(epoch,train_dice,'b',label='Training dice coef')\n","ax[2].plot(epoch,val_dice,'r',label='Validation dice coef')\n","ax[2].legend()\n","\n","ax[3].plot(epoch,hist['mean_io_u'],'b',label='Training mean IOU')\n","ax[3].plot(epoch,hist['val_mean_io_u'],'r',label='Validation mean IOU')\n","ax[3].legend()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Prediction examples "]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["# mri type must one of 1) flair 2) t1 3) t1ce 4) t2 ------- or even 5) seg\n","# returns volume of specified study at `path`\n","def imageLoader(path):\n","    image = nib.load(path).get_fdata()\n","    X = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, self.n_channels))\n","    for j in range(VOLUME_SLICES):\n","        X[j +VOLUME_SLICES*c,:,:,0] = cv2.resize(image[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n","        X[j +VOLUME_SLICES*c,:,:,1] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n","\n","        y[j +VOLUME_SLICES*c] = seg[:,:,j+VOLUME_START_AT];\n","    return np.array(image)\n","\n","\n","# load nifti file at `path`\n","# and load each slice with mask from volume\n","# choose the mri type & resize to `IMG_SIZE`\n","def loadDataFromDir(path, list_of_files, mriType, n_images):\n","    scans = []\n","    masks = []\n","    for i in list_of_files[:n_images]:\n","        fullPath = glob.glob( i + '/*'+ mriType +'*')[0]\n","        currentScanVolume = imageLoader(fullPath)\n","        currentMaskVolume = imageLoader( glob.glob( i + '/*seg*')[0] ) \n","        # for each slice in 3D volume, find also it's mask\n","        for j in range(0, currentScanVolume.shape[2]):\n","            scan_img = cv2.resize(currentScanVolume[:,:,j], dsize=(IMG_SIZE,IMG_SIZE), interpolation=cv2.INTER_AREA).astype('uint8')\n","            mask_img = cv2.resize(currentMaskVolume[:,:,j], dsize=(IMG_SIZE,IMG_SIZE), interpolation=cv2.INTER_AREA).astype('uint8')\n","            scans.append(scan_img[..., np.newaxis])\n","            masks.append(mask_img[..., np.newaxis])\n","    return np.array(scans, dtype='float32'), np.array(masks, dtype='float32')\n","        \n","#brains_list_test, masks_list_test = loadDataFromDir(VALIDATION_DATASET_PATH, test_directories, \"flair\", 5)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def predictByPath(case_path,case):\n","    files = next(os.walk(case_path))[2]\n","    X = np.empty((VOLUME_SLICES, IMG_SIZE, IMG_SIZE, 2))\n","  #  y = np.empty((VOLUME_SLICES, IMG_SIZE, IMG_SIZE))\n","    \n","    vol_path = os.path.join(case_path, f'BraTS20_Training_{case}_flair.nii');\n","    flair=nib.load(vol_path).get_fdata()\n","    \n","    vol_path = os.path.join(case_path, f'BraTS20_Training_{case}_t1ce.nii');\n","    ce=nib.load(vol_path).get_fdata() \n","    \n"," #   vol_path = os.path.join(case_path, f'BraTS20_Training_{case}_seg.nii');\n"," #   seg=nib.load(vol_path).get_fdata()  \n","\n","    \n","    for j in range(VOLUME_SLICES):\n","        X[j,:,:,0] = cv2.resize(flair[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n","        X[j,:,:,1] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n"," #       y[j,:,:] = cv2.resize(seg[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n","        \n","  #  model.evaluate(x=X,y=y[:,:,:,0], callbacks= callbacks)\n","    return model.predict(X/np.max(X), verbose=1)\n","\n","\n","def showPredictsById(case, start_slice = 60):\n","    path = f\"../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_{case}\"\n","    gt = nib.load(os.path.join(path, f'BraTS20_Training_{case}_seg.nii')).get_fdata()\n","    origImage = nib.load(os.path.join(path, f'BraTS20_Training_{case}_flair.nii')).get_fdata()\n","    p = predictByPath(path,case)\n","\n","    core = p[:,:,:,1]\n","    edema= p[:,:,:,2]\n","    enhancing = p[:,:,:,3]\n","\n","    plt.figure(figsize=(18, 50))\n","    f, axarr = plt.subplots(1,6, figsize = (18, 50)) \n","\n","    for i in range(6): # for each image, add brain background\n","        axarr[i].imshow(cv2.resize(origImage[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)), cmap=\"gray\", interpolation='none')\n","    \n","    axarr[0].imshow(cv2.resize(origImage[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)), cmap=\"gray\")\n","    axarr[0].title.set_text('Original image flair')\n","    curr_gt=cv2.resize(gt[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE), interpolation = cv2.INTER_NEAREST)\n","    axarr[1].imshow(curr_gt, cmap=\"Reds\", interpolation='none', alpha=0.3) # ,alpha=0.3,cmap='Reds'\n","    axarr[1].title.set_text('Ground truth')\n","    axarr[2].imshow(p[start_slice,:,:,1:4], cmap=\"Reds\", interpolation='none', alpha=0.3)\n","    axarr[2].title.set_text('all classes')\n","    axarr[3].imshow(edema[start_slice,:,:], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n","    axarr[3].title.set_text(f'{SEGMENT_CLASSES[1]} predicted')\n","    axarr[4].imshow(core[start_slice,:,], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n","    axarr[4].title.set_text(f'{SEGMENT_CLASSES[2]} predicted')\n","    axarr[5].imshow(enhancing[start_slice,:,], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n","    axarr[5].title.set_text(f'{SEGMENT_CLASSES[3]} predicted')\n","    plt.show()\n","    \n","    \n","showPredictsById(case=test_ids[0][-3:])\n","showPredictsById(case=test_ids[1][-3:])\n","showPredictsById(case=test_ids[2][-3:])\n","showPredictsById(case=test_ids[3][-3:])\n","showPredictsById(case=test_ids[4][-3:])\n","showPredictsById(case=test_ids[5][-3:])\n","showPredictsById(case=test_ids[6][-3:])\n","\n","\n","# mask = np.zeros((10,10))\n","# mask[3:-3, 3:-3] = 1 # white square in black background\n","# im = mask + np.random.randn(10,10) * 0.01 # random image\n","# masked = np.ma.masked_where(mask == 0, mask)\n","\n","# plt.figure()\n","# plt.subplot(1,2,1)\n","# plt.imshow(im, 'gray', interpolation='none')\n","# plt.subplot(1,2,2)\n","# plt.imshow(im, 'gray', interpolation='none')\n","# plt.imshow(masked, 'jet', interpolation='none', alpha=0.7)\n","# plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true},"outputs":[],"source":["case = case=test_ids[3][-3:]\n","path = f\"../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_{case}\"\n","gt = nib.load(os.path.join(path, f'BraTS20_Training_{case}_seg.nii')).get_fdata()\n","p = predictByPath(path,case)\n","\n","\n","core = p[:,:,:,1]\n","edema= p[:,:,:,2]\n","enhancing = p[:,:,:,3]\n","\n","\n","i=40 # slice at\n","eval_class = 2 #     0 : 'NOT tumor',  1 : 'ENHANCING',    2 : 'CORE',    3 : 'WHOLE'\n","\n","\n","\n","gt[gt != eval_class] = 1 # use only one class for per class evaluation \n","\n","resized_gt = cv2.resize(gt[:,:,i+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE))\n","\n","plt.figure()\n","f, axarr = plt.subplots(1,2) \n","axarr[0].imshow(resized_gt, cmap=\"gray\")\n","axarr[0].title.set_text('ground truth')\n","axarr[1].imshow(p[i,:,:,eval_class], cmap=\"gray\")\n","axarr[1].title.set_text(f'predicted class: {SEGMENT_CLASSES[eval_class]}')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["model.compile(loss=\"categorical_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics = ['accuracy',tf.keras.metrics.MeanIoU(num_classes=4), dice_coef, precision, sensitivity, specificity, dice_coef_necrotic, dice_coef_edema, dice_coef_enhancing] )\n","# Evaluate the model on the test data using `evaluate`\n","print(\"Evaluate on test data\")\n","results = model.evaluate(test_generator, batch_size=100, callbacks= callbacks)\n","print(\"test loss, test acc:\", results)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
